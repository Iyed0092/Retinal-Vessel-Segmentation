{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f37a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION\n",
    "def alpaca_model(image_shape=IMG_SIZE, data_augmentation=data_augmenter()):\n",
    "    ''' Define a tf.keras model for binary classification out of the MobileNetV2 model\n",
    "    Arguments:\n",
    "        image_shape -- Image width and height\n",
    "        data_augmentation -- data augmentation function\n",
    "    Returns:\n",
    "    Returns:\n",
    "        tf.keras.model\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    input_shape = image_shape + (3,)\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    \n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape,\n",
    "                                                   include_top=False, # <== Important!!!!\n",
    "                                                   weights='imagenet') # From imageNet\n",
    "    \n",
    "    # Freeze the base model by making it non trainable\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # create the input layer (Same as the imageNetv2 input size)\n",
    "    inputs = tf.keras.Input(shape=input_shape) \n",
    "    \n",
    "    # apply data augmentation to the inputs\n",
    "    x = data_augmentation(inputs)\n",
    "    \n",
    "    # data preprocessing using the same weights the model was trained on\n",
    "    # Already Done -> preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "    x = preprocess_input(x) \n",
    "    \n",
    "    # set training to False to avoid keeping track of statistics in the batch norm layer\n",
    "    x = base_model(x, training=False) \n",
    "    \n",
    "    # Add the new Binary classification layers\n",
    "    # use global avg pooling to summarize the info in each channel\n",
    "    x = tfl.GlobalAveragePooling2D()(x) \n",
    "    #include dropout with probability of 0.2 to avoid overfitting\n",
    "    x = tfl.Dropout(0.2)(x)\n",
    "        \n",
    "    # create a prediction layer with one neuron (as a classifier only needs one)\n",
    "    prediction_layer = tfl.Dense(1)\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    \n",
    "    outputs = prediction_layer(x) \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc31373",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = alpaca_model(IMG_SIZE, data_augmentation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e1fba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.01\n",
    "model2.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c6c030",
   "metadata": {},
   "source": [
    "### Finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f50bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3\n",
    "\n",
    "base_model.trainable = True\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 126\n",
    "\n",
    "### START CODE HERE\n",
    "\n",
    "##### One Way ########\n",
    "\"\"\"\n",
    "for layer in base_model.layers:\n",
    "    if layer.name == 'block_16_expand':\n",
    "        break\n",
    "    layer.trainable = False\n",
    "    print('Layer ' + layer.name + ' frozen.')\n",
    "\"\"\"\n",
    "######## Other Way ######\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    #print('Layer ' + layer.name + ' frozen.')\n",
    "    layer.trainable = None\n",
    "    \n",
    "# Define a BinaryCrossentropy loss function. Use from_logits=True\n",
    "loss_function= tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "# Define an Adam optimizer with a learning rate of 0.1 * base_learning_rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate*0.1)# 0.001\n",
    "# Use accuracy as evaluation metric\n",
    "metrics=['accuracy']\n",
    "\n",
    "### END CODE HERE\n",
    "\n",
    "model2.compile(loss=loss_function,\n",
    "              optimizer = optimizer,\n",
    "              metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e57cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 5\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model2.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=validation_dataset)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
